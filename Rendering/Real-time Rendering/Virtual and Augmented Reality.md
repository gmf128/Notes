# Chapter 21: Virtual and Augmented Reality

## 21.1 Equipment and System Overview

 除了CPU和GPU之外，**用于图形的虚拟和增强现实设备可以分为传感器和显示器两类**。传感器包括用于检测用户旋转和位置的跟踪器，以及大量的输入方法和设备。对于显示器，一些系统依赖于使用手机屏幕，逻辑上被分割为两半。专用系统通常有两个独立的显示器。**显示器是用户在虚拟现实系统中看到的一切**。对于增强现实，**通过使用专门设计的光学元件，将虚拟信息与真实世界的视图相结合。** 

**VR**

 手机可以用于沉浸式体验，有时表现得令人惊讶地出色。手机可以放置在头戴显示器（HMD）中，从简单的查看器（如Google Cardboard）到无需双手操作且提供额外输入设备的设备（如GearVR）。手机的方向传感器（重力、磁北极等）可以确定显示器的方向。方向，也称为姿态，有三个自由度，例如偏航、俯仰和翻滚，如4.2.1节所讨论的。API可以将方向以欧拉角、旋转矩阵或四元数的形式返回。固定视角全景图和视频等现实世界内容可以与这些设备很好地配合，因为根据用户的方向呈现正确的二维视图的成本相对较低 

 移动设备相对较为有限的计算能力以及使用GPU和CPU硬件时的电力需求对其功能进行了限制。而系联(Tethered)虚拟现实设备通过一组线缆将用户的头戴式显示器连接到固定的计算机，限制了移动性，但可以使用更强大的处理器。

>   We will briefly describe the sensors for just two systems, the Oculus Rift and the HTC Vive. Both provide six degrees of freedom (6-DOF) tracking: orientation and position. The Rift tracks the location of the HMD and controllers by up to three separate infrared cameras. When the headset’s position is determined by stationary external sensors, this is called outside-in tracking. An array of infrared LEDs on the outside of the headset allow it to be tracked. The Vive uses a pair of “lighthouses” that shine non-visible light into a room at rapid intervals, which sensors in the headset and controllers detect in order to triangulate their positions. This is a form of inside-out tracking, where the sensors are part of the HMD. 

 我们将简要介绍两个系统的传感器，Oculus Rift和HTC Vive的传感器。这两个传感器都提供六自由度（**6-DOF ：6 degrees of freedom**）的跟踪：方向和位置。Rift通过最多三个单独的红外摄像头来跟踪HMD（头戴显示器）和控制器的位置。当头戴式显示器的位置是由固定的外部传感器确定时，这被称为外部跟踪。头戴式显示器外部的红外LED阵列可以跟踪其位置。Vive使用一对“灯塔”，在房间内以快速间隔发出不可见光，头戴式显示器和控制器中的传感器探测到这些光线，并通过三角测量确定它们的位置。这是一种内部跟踪的形式，其中传感器是头戴式显示器的一部分。 

 手柄控制器是标准的设备，可以被跟踪并与用户一起移动，不同于鼠标和键盘。许多其他类型的输入设备已经基于各种技术为虚拟现实开发出来。这些设备包括手套或其他肢体或身体跟踪设备、眼球跟踪设备以及模拟原地移动的设备，例如压力垫、单向或全向踏步机、静止自行车和人尺寸的仓鼠球等。除了光学系统之外，还对基于磁性、惯性、机械、深度检测和声学现象的跟踪方法进行了探索 

**AR/MR**

增强现实被定义为将计算机生成的内容与用户的现实世界视图相结合。任何提供悬浮显示（HUD）并在图像上叠加文本数据的应用程序都是增强现实的基本形式。例如，2009年推出的Yelp Monocle会在相机视图上叠加商家用户评级和距离信息。Google翻译的移动版可以将标志替换为翻译的等效物。例如，游戏Pokémon GO会在真实环境中叠加虚构的生物。Snapchat可以检测面部特征并添加服装元素或动画。

在合成渲染方面更有趣的是混合现实（MR），它是增强现实的子集，在其中真实世界和三维虚拟内容可以在实时中相互混合和交互[1570]。混合现实的一个经典应用案例是在手术中，将患者器官的扫描数据与外部身体的相机视图进行融合。这种情况假设了一个具有相当大的计算能力和精确性的绑定系统。另一个例子是与虚拟袋鼠玩“标签游戏”，在这个游戏中，房子的真实墙壁可以隐藏你的对手。在这种情况下，移动性更为重要，而注册或其他因素对质量的影响相对较小

在这个领域使用的一种技术是在头戴式显示器的前部安装一个视频摄像头。例如，每个HTC Vive都装有一个前置摄像头，开发者可以访问它。**这个世界的视图将被发送到用户的眼睛，并且可以将合成的图像与其叠加。这有时被称为透传式增强现实或虚拟现实，或者是介入式现实[489]，其中用户并不直接观察环境(Apple Vision Pro)**。使用这样的视频流的一个优点是可以更好地控制将虚拟对象与真实对象合并的过程。缺点是实际世界的感知会有一些延迟。Vrvana的Totem和Occipital的Bridge是使用这种类型排列的头戴式显示器的增强现实系统的示例。

 截至本书编写时，微软的HoloLens是最著名的混合现实系统。它是一个无线系统，内置了CPU、GPU以及微软所称的HPU（全息处理单元）的一体式头戴式显示器。HPU是一个定制芯片，由24个数字信号处理核心组成，功耗不到10瓦。这些核心用于处理类似Kinect的摄像头从环境中获取的世界数据。除了加速计等其他传感器外，这个视图内外跟踪，而且不需要额外使用光站、QR码（又称标记）或其他外部元素，这是额外的优势。

## 21.2 Physical Elements

### 21.2.1 Latency

 减轻延迟效应在虚拟现实（VR）和增强现实（AR）系统中尤为重要，通常是最关键的问题[5, 228]。我们在第三章中讨论了GPU如何隐藏内存延迟。这种延迟类型由纹理获取等操作引起，仅影响整个系统的一小部分。在这里，我们指的是整个系统的“运动到光子”延迟。也就是说，假设你开始向左转头。从你的头部朝向特定方向，到从该方向生成的视图被显示出来，经过了多少时间？从用户输入的检测（例如，头部方向）到响应（显示新的图像）的每个硬件组件的处理和通信成本，都会累积成数十毫秒的延迟。 

 在普通显示器（即不附着在脸上的显示器）的系统中，延迟最多只会令人感到恼人，破坏了互动和连接的感觉。**在增强现实和混合现实应用中，较低的延迟将有助于增加“像素粘着”，即虚拟场景中的虚拟对象与现实世界保持连接的程度**。系统中的延迟越大，虚拟对象相对于其真实世界的对应物就会出现更多漂移或漂浮的现象。在具有沉浸式虚拟现实的情况下，显示器是唯一的视觉输入，**延迟可能会产生更严重的影响**。虽然不是真正的疾病，但被称为仿真疾病，可能会引起出汗、头晕、恶心等症状，甚至更严重。如果你开始感到不适，立即摘下头戴显示设备（HMD）- 你无法通过忍受这种不适来缓解症状，只会让病情加重[1183]。引用 Carmack [650]的话说：“别逞能。我们不希望在演示室里清理呕吐物。”实际上，实际呕吐的情况很少见，但其影响仍可能严重和令人无法忍受，并且可能持续一整天。 

 在虚拟现实中，仿真疾病是指显示的图像与用户通过其他感官（如内耳前庭系统用于平衡和运动）的期望或感知不匹配。头部运动和显示图像之间的延迟越低，效果越好。**一些研究指出，15毫秒的延迟是无法察觉的。超过20毫秒的延迟肯定可以被察觉，并且会产生不良影响**[5, 994, 1311]。作为比较，从鼠标移动到显示器，电子游戏通常具有50毫秒或更长的延迟，关闭垂直同步（Section 23.6.2）可以达到30毫秒。**虚拟现实系统通常采用90帧/秒的显示频率，这对应于每帧11.1毫秒的时间。**在典型的桌面系统中，通过电缆将帧扫描到显示器需要约11毫秒的时间，因此即使您能在1毫秒内渲染，仍然会有12毫秒的延迟。 

 追踪姿态(**Tracking pose**)，简称姿态，是观察者头部在真实世界中的方向和（如果可用）位置。姿态用于形成进行渲染所需的相机矩阵。在帧开始时，可以使用粗略的姿态预测来执行模拟，例如检测角色和环境中元素的碰撞。当即将开始渲染时，可以在那一刻检索到更新的姿态预测，并用于更新相机的视角。这个预测会更加准确，因为它是在稍后检索到的，且时间更短。当图像即将显示时，可以检索到另一个更准确的姿态预测，并将该图像扭曲以更好地匹配用户的位置。每个较后的预测不能完全补偿基于早期不准确预测的计算，但尽可能地使用它们可以显著改善整体体验。各种设备的硬件增强提供了快速查询和获取在需要的时刻更新的头部姿态的能力。 

### 21.2.2 Optics

 设计精确的物理光学系统，将头戴式显示器的内容映射到视网膜上对应的位置，是一项昂贵的任务。虚拟现实显示系统之所以能够价格合理，是因为由GPU生成的图像经过独立的后处理，以便正确地抵达我们的眼睛 

<img src="Virtual%20and%20Augmented%20Reality.assets/1688126684887.png" alt="1688126684887" style="zoom: 67%;" />

### 21.2.3 Stereopsis(立体视觉)

 “Stereopsis”（立体视觉）是指我们大脑通过合并每只眼睛看到的略有不同的图像，从而创造出围绕我们的世界的深度感知或三维结构的视觉过程。它是我们准确感知和判断距离的基础。立体视觉成立的原因是我们的眼睛有水平间隔，每只眼睛从稍微不同的视角观察物体。 

值得注意的是，双目视觉并非感知深度的唯一线索。 物体大小、纹理图案变化、阴影、相对运动（视差）以及其他视觉深度线索只需要一个眼睛就可以起作用。 

Vergence是用来表示 两只眼睛旋转多少来观察一个物体 。Convergence(收敛)表示两眼向内运动以聚焦一个物体。Divergence(发散)表示两眼向外运动以观察远处的物体。 观察远处物体的视线在效果上是平行的。 

<img src="Virtual%20and%20Augmented%20Reality.assets/1688127785404.png" alt="1688127785404" style="zoom:50%;" />

 眼睛必须调整形状以使物体清晰可见的程度被称为accommodative demand。例如，Oculus Rift的光学系统相当于观看距离用户约1.3米的屏幕。眼睛需要向内转动的程度以聚焦一个物体被称为vergence demand，可以参考图21.4。在现实世界中，眼睛会同时改变晶状体的形状并向内转动，这个现象被称为accommodation-convergence reflex。在显示屏上， accommodative
demand 是恒定的，但是随着眼睛对不同**感知**深度（ **perceived** depths ）的物体进行聚焦， vergence demand 会发生变化。这种不匹配可能会导致眼睛疲劳，因此Oculus建议用户长时间观看的物体距离用户大约0.75到3.5米之间[1311，1802]。这种不匹配在一些增强现实系统中也会产生知觉效应，例如，用户可能会在现实世界中聚焦于远处的物体，但是之后必须重新聚焦于与之相关的固定深度靠近眼睛的虚拟广告牌上。一些研究团队正在研发能够根据用户的眼动调整知觉焦距的硬件，有时被称为自适应焦点或多焦显示[976、1186、1875]。 

 为虚拟现实(VR)和增强现实(AR)生成立体对的规则与为单显示系统生成立体对的规则不同。在单显示系统中，某些技术（偏振镜片、快门眼镜、多视角显示光学）将独立的图像从同一屏幕呈现给每只眼睛。而在虚拟现实中，每只眼睛都有独立的显示器，这意味着每只眼睛必须以一种方式定位，使得投射到视网膜上的图像与实际情况接近。**眼睛之间的距离被称为瞳孔间距（interpupillary distance，IPD）**。在对4000名美国陆军士兵进行的一项研究中，发现其**IPD范围从52毫米到78毫米不等，平均为63.5毫米** [1311]。虚拟现实和增强现实系统都有校准方法来确定和调整用户的IPD，从而提高图像质量和舒适度。系统的应用程序接口(API)控制一个包括IPD在内的相机模型。最好避免修改用户的感知IPD以达到某种效果。例如，增加眼睛间距可能会增强深度的感知，但也可能导致眼睛疲劳 

## 21.3 APIs and Hardware

 首先要明确一点：除非你有充分的理由，否则应始终使用系统提供的虚拟现实软件开发工具包（SDK）和应用程序编程接口（API）。例如，你可能认为自己的畸变着色器更快且效果看起来还不错。然而，在实际应用中，它很可能导致用户严重的不适感，除非经过广泛的测试，否则你无法确切知道这是否属实。出于这个原因和其他原因，所有主要的API都已不再支持应用程序控制的畸变处理；正确实现VR显示是一个系统级任务。为了优化性能和保持质量，已经有很多精心的工程设计代表你进行了处理。本节将讨论各个供应商的SDK和API提供了哪些支持 

### 21.3.2  Foveated Rendering 

为了理解这种渲染技术，我们需要对我们的眼睛有一些更多的了解。

**Fovea是我们每只眼睛视网膜上的一个小凹陷，其中密集分布着锥细胞，这些是与彩色视觉相关的感光细胞。我们在这个区域的视力最高，我们通过转动眼睛来利用这一能力**，例如追踪飞行中的鸟类或阅读页面上的文字。视力在距离Fovea中心每2.5度的范围内迅速下降，前30度每减少2.5度，视力会下降约50%，而远离Fovea中心的范围下降更快。**我们的眼睛具有适用于双眼视觉（两只眼睛可以看到同一个物体）的水平视野，约为114度。**第一代消费者头戴式显示设备的视野稍小，两只眼睛的水平视野约为80至100度，而这个数字可能会增加。对于2016年的头戴式显示设备，中心20度视野的面积约占显示屏的3.6%，而预计到2020年左右，这个比例会降低到2% [1357]。在这期间，显示器的分辨率可能会提高一个数量级 [8]。

 在低视觉敏锐度区域，眼睛所看到的显示屏像素绝大部分都提供了利用Forveated渲染（foveated rendering）来减少工作量的机会[619, 1358]。**这个想法是以高分辨率和高质量渲染眼睛注视的区域，而在其他区域消耗较少的资源。然而，问题在于眼睛是会移动的，所以要渲染的区域会不断变化。例如，在观察对象时，眼睛会进行一系列称为扫视的快速移动，速度可高达每秒900度，也就是在一个90帧每秒的系统中，可能每帧移动10度**。精确的眼动追踪硬件有可能通过在Forvea区域外执行较少的渲染工作来大幅提升性能，但这些传感器存在技术挑战[8]。此外，以“放大”像素在外围区域往往会增加混叠问题。通过尝试保持对比度并避免随时间发生大的变化，可以改善较低分辨率的外围区域的渲染，并使这些区域更具感知可接受性[1357]。Stengel等人[1697]讨论了以前的Forveated渲染方法，以减少片段着色器调用的数量，并提出了他们自己的方法 

