{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 1.Derivatives and Gradients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Directional derivatives(Why gradients are important?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directional derivative $∇s f(\\mathbf{x})$ of a multivariate function $f$ is the instantaneous rate of change of $f(\\mathbf{x})$ as $\\mathbf{x}$ is moved with velocity $\\mathbf{s}$. The definition is closely related\n",
    "to the definition of a derivative of a univariate function:\n",
    "$$∇s f(\\mathbf{x}) = \\lim_{h\\to 0} \\frac{f(\\mathbf{x}+h\\mathbf{s})-f(\\mathbf{x})}{h}\n",
    "\\\\=\\lim_{h\\to 0} \\frac{f(\\mathbf{x}+h\\mathbf{s})-f(\\mathbf{x}+h\\mathbf{s_x})}{h||\\mathbf{s_y}||}\\cdot ||\\mathbf{s_y}||+\\lim_{h\\to 0} \\frac{f(\\mathbf{x}+h\\mathbf{s_x})-f(\\mathbf{x})}{h\\mathbf{||s_x||}}\\cdot ||s_x||\n",
    "\\\\=\\nabla{f(\\mathbf{x})}\\cdot \\mathbf{s} \\tag{1.1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Cauchy-Shwarz inequation:\n",
    "$$\\nabla{f(\\mathbf{x})}\\cdot \\mathbf{s} \\le \\Vert\\nabla{f(\\mathbf{x})}\\Vert\\cdot \\Vert\\mathbf{s}\\Vert\\tag{1.2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The directional derivative is highest in the gradient direction, and it is lowest\n",
    "in the direction opposite the gradient. This directional dependence arises from\n",
    "the dot product in the directional derivative’s definition and from the fact that\n",
    "the gradient is a local tangent hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Numerical Derivatives\n",
    "\n",
    "### 1.2.1 Finite Difference Methods\n",
    "\n",
    "As the name implies, finite difference methods compute the difference between two\n",
    "values that differ by a finite step size. They approximate the derivative definitions\n",
    "in equation  using small differences:\n",
    "\n",
    "$$f^{\\prime}(x) \\approx \\frac{f(x+h)-f(x)}{h} \\approx \\frac{f(x)-f(x-h)}{h} \\approx \\frac{f(x+\\frac{1}{2}h)-f(x-\\frac{1}{2}h)}{h} \\tag{1.3}$$\n",
    "\n",
    "They are called w.r.t. forward difference, backward difference, and central difference.\n",
    "Using the Taylor expansion,we will derive that the forward and backward difference estimator has an error term $O(h)$,while the central differenve estimator has an error term $O(h^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that whenever a function is differentiable, it is continuous:\n",
    "$$\\lim_{h\\to 0}f(x+h)-f(x)=0$$\n",
    "In computers, there exists *subtractive cancellation erro* due to the machine precision for floating\n",
    "point values, which causes that $f(x+h)-f(x)=0$while h is not zero.So, too small h will also bring up some problems, while too large h implies\n",
    "too large error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Complex Step Method\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
